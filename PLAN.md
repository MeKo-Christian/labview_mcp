# Repository Transformation Plan

## üéØ Project Goal

**Transform this repository (`labview_mcp`) from a Python-based MCP server to a Go-based MCP server** while maintaining all existing LabVIEW integration capabilities. This transformation will improve performance, deployment simplicity, and maintainability.

### Current State
- Repository: `github.com/MeKo-Christian/labview_mcp`
- Implementation: Python-based MCP server with LabVIEW COM integration
- Tools: Existing LabVIEW Request VIs and DQMH modules

### Target State
- Same repository, restructured with Go as primary implementation
- Python code maintained temporarily for reference and gradual migration
- New directory structure to support Go development
- Improved IPC abstraction for LabVIEW communication

---

## Overall architecture idea

Target structure for this repository:

```text
labview_mcp/                    # THIS REPOSITORY
  # Existing Python implementation (maintained during migration)
  main.py
  LabVIEW_Server/

  # New Go implementation (to be created)
  cmd/
    mcp-server/                 # main.go -> MCP entrypoint
    labview-bridge/             # optional: small helper / stub for IPC PoC
  internal/
    mcpserver/                  # Go MCP server setup (tools, schema)
    ipc/                        # IPC abstraction (PoC first, real later)
    logging/
  docs/
    tools.md                    # mapping of LabVIEW Request VIs <-> tools

  # Build artifacts
  bin/

  go.mod
  go.sum
```

**Migration Strategy:** Python and Go will coexist in this repository during the transition. Once Go implementation is stable and tested, Python code will be marked as deprecated or archived.

---

## Track A ‚Äì Replace the MCP part with Go (primary goal)

### Phase A1 ‚Äì Set up the Go project & skeleton MCP server

**Goal:** A Go binary that Claude/Desktop can launch as an MCP server, with one trivial tool.

#### Todo List - Phase A1

- [x] **Task 1.1: Initialize Go module**
  - [x] Run `go mod init github.com/MeKo-Christian/labview_mcp` (using this repo name)
  - [x] Verify `go.mod` file is created correctly
  - [x] Add dependency: `go get github.com/modelcontextprotocol/go-sdk/mcp`

- [x] **Task 1.2: Create directory structure**
  - [x] Create `cmd/mcp-server/` directory
  - [x] Create `internal/` directory
  - [x] Create `internal/mcpserver/` directory
  - [x] Create `docs/` directory if it doesn't exist

- [x] **Task 1.3: Implement minimal MCP server**
  - [x] Create `cmd/mcp-server/main.go` file
  - [x] Implement `mcp.NewServer(...)` setup
  - [x] Add a single dummy tool: `ping` or `echo`
  - [x] Configure server to run with `server.Run(context.Background(), &mcp.StdioTransport{})`
  - [x] Test compilation with `go build ./cmd/mcp-server`

   Example skeleton:

   ```go
   package main

   import (
       "context"
       "log"

       "github.com/modelcontextprotocol/go-sdk/mcp"
   )

   type PingInput struct {
       Message string `json:"message" jsonschema:"Message to echo"`
   }

   type PingOutput struct {
       Reply string `json:"reply" jsonschema:"Echoed reply"`
   }

   func PingTool(
       ctx context.Context,
       req *mcp.CallToolRequest,
       input PingInput,
   ) (*mcp.CallToolResult, PingOutput, error) {
       return nil, PingOutput{Reply: "Pong: " + input.Message}, nil
   }

   func main() {
       server := mcp.NewServer(&mcp.Implementation{
           Name:    "labview-mcp-go",
           Version: "0.1.0",
       }, nil)

       mcp.AddTool(server, &mcp.Tool{
           Name:        "ping",
           Description: "Echo test tool",
       }, PingTool)

       if err := server.Run(context.Background(), &mcp.StdioTransport{}); err != nil {
           log.Fatal(err)
       }
   }
   ```

- [ ] **Task 1.4: Build the MCP server binary**
  - [ ] Create `bin/` directory for build artifacts
  - [ ] Build Windows executable: `GOOS=windows GOARCH=amd64 go build -o bin/mcp-server.exe ./cmd/mcp-server`
  - [ ] Build Linux executable (if needed): `go build -o bin/mcp-server ./cmd/mcp-server`
  - [ ] Verify binary runs without errors: `./bin/mcp-server --help` or similar

- [ ] **Task 1.5: Integrate with Claude Desktop**
  - [ ] Locate Claude Desktop config file (`claude_desktop_config.json`)
  - [ ] Add entry for `labview_mcp_go` server
  - [ ] Configure correct path to built binary
  - [ ] Restart Claude Desktop to load new configuration

   Example config entry:

     ```json
     {
       "mcpServers": {
         "labview_mcp_go": {
           "command": "C:\\path\\to\\labview_mcp\\bin\\mcp-server.exe",
           "args": []
         }
       }
     }
     ```

- [ ] **Task 1.6: Smoke test**
  - [ ] Start Claude Desktop
  - [ ] Check server connection status in Claude
  - [ ] Ask Claude to call the `ping` tool
  - [ ] Verify response is received correctly
  - [ ] Check for any error logs or issues

**Exit criteria Phase A1:**
‚úÖ You can call the Go MCP server from Claude (or your host) and get responses from at least one dummy tool.

---

### Phase A2 ‚Äì Extract tool schema from current system

**Goal:** Clearly define which tools you need in Go and their schemas.

#### Todo List - Phase A2

- [x] **Task 2.1: Analyze existing Python MCP implementation**
  - [x] Read through `main.py` to identify all registered tools
  - [x] List tool names (e.g., `start_module`, `stop_module`, `call_request`)
  - [x] Document each tool's purpose and description
  - [x] Extract input parameter definitions for each tool
  - [x] Extract output/return value definitions for each tool

- [ ] **Task 2.2: Examine LabVIEW VIs structure**
  - [ ] Explore `LabVIEW_Server/` directory
  - [ ] Identify Request VIs that correspond to Python tools
  - [ ] Document VI input/output specifications
  - [ ] Map Python tools to their corresponding LabVIEW VIs

- [x] **Task 2.3: Create tool schema documentation**
  - [x] Create `docs/tools.md` file
  - [x] Document each tool with complete schema
  - [x] Include tool name, description, inputs, outputs for each
  - [x] Add notes about optional vs. required parameters
  - [x] Alternatively, create `docs/tools.json` with structured data

   Example format for `docs/tools.json`:

   ```json
   [
     {
       "name": "start_module",
       "description": "Start the LabVIEW DQMH server module.",
       "inputs": [
         { "name": "module_name", "type": "string", "description": "Name of the module" }
       ],
       "outputs": [
         { "name": "status", "type": "string" }
       ]
     }
   ]
   ```

- [ ] **Task 2.4: Prioritize tools for v1 implementation**
  - [ ] Review all documented tools
  - [ ] Select 2-3 core tools for initial Go implementation
  - [ ] Document the rationale for tool selection (most used, simplest, etc.)
  - [ ] Create implementation priority list
  - [ ] Update `docs/tools.md` or `docs/tools.json` with v1 markers

**Exit criteria Phase A2:**
‚úÖ You have a clear list of tools & schemas, and you know which ones you'll implement first in Go.

---

### Phase A3 ‚Äì Implement Go tools as stubs (no LabVIEW yet)

**Goal:** Go MCP server exposes the real tools, but their implementation is still stubbed / fake.

#### Todo List - Phase A3

- [ ] **Task 3.1: Create tool implementation structure**
  - [ ] Create `internal/mcpserver/tools.go` file
  - [ ] Set up package structure for tool definitions
  - [ ] Import necessary MCP SDK packages

- [ ] **Task 3.2: Implement first v1 tool as stub (e.g., start_module)**
  - [ ] Define input struct with proper JSON tags and jsonschema tags
  - [ ] Define output struct with proper JSON tags and jsonschema tags
  - [ ] Create handler function matching MCP signature
  - [ ] Implement stub logic (return fake/placeholder data)
  - [ ] Register tool with `mcp.AddTool()` in main.go
  - [ ] Add descriptive tool name and description

   Example for `start_module`:

   ```go
   type StartModuleInput struct {
       ModuleName string `json:"module_name" jsonschema:"Name of the LabVIEW module to start"`
   }

   type StartModuleOutput struct {
       Status string `json:"status" jsonschema:"Status of the startup request"`
   }

   func StartModuleTool(
       ctx context.Context,
       req *mcp.CallToolRequest,
       input StartModuleInput,
   ) (*mcp.CallToolResult, StartModuleOutput, error) {
       // For now: stub
       return nil, StartModuleOutput{
           Status: "stub: would start " + input.ModuleName,
       }, nil
   }
   ```

- [ ] **Task 3.3: Implement remaining v1 tools as stubs**
  - [ ] Repeat Task 3.2 for the second v1 tool
  - [ ] Repeat Task 3.2 for the third v1 tool
  - [ ] Ensure consistent naming and structure across all tools
  - [ ] Test compilation after each tool addition

- [ ] **Task 3.4: Build and deploy updated server**
  - [ ] Build updated binary: `go build -o bin/mcp-server.exe ./cmd/mcp-server`
  - [ ] Replace old binary in Claude Desktop configuration
  - [ ] Restart Claude Desktop

- [ ] **Task 3.5: Verify in Claude Desktop**
  - [ ] Check that all v1 tools show up in Claude's tool list
  - [ ] Verify tools have proper descriptions
  - [ ] Verify tools have correct parameter schemas
  - [ ] Call each tool and verify stub responses are returned
  - [ ] Test error handling (invalid inputs, etc.)

**Exit criteria Phase A3:**
‚úÖ From the LLM's perspective, the Go server is a drop-in replacement for the Python one (same tools & schemas), even though they don't yet talk to LabVIEW.

At this point, **Track A's primary goal (Go MCP instead of Python MCP) is basically achieved for the LLM side**. LabVIEW integration is now "just" an implementation detail behind those tools.

---

## Track B ‚Äì PoC IPC ‚Äúthing that spits out IPC‚Äù

Here we decouple *how* Go talks to LabVIEW, and start with a PoC.

### Design decision: IPC for PoC

For a PoC, I‚Äôd recommend:

* **Transport:** line-delimited JSON over stdin/stdout or a simple TCP port on `localhost`.
* **Message model:**

  * `Request` (Go -> LabVIEW bridge)
  * `Response` (LabVIEW bridge -> Go)

Example JSON:

```json
{
  "type": "request",
  "id": "c0f2a754-3c26-4f17-8ac7-312de90f1e84",
  "tool": "start_module",
  "payload": {
    "module_name": "MyModule"
  }
}
```

```json
{
  "type": "response",
  "id": "c0f2a754-3c26-4f17-8ac7-312de90f1e84",
  "payload": {
    "status": "ok"
  },
  "error": null
}
```

For the PoC, the ‚ÄúLabVIEW bridge‚Äù doesn‚Äôt need to be LabVIEW yet. It can be:

* A tiny helper process (Go or Python) that reads these JSON lines, logs them, and sends canned responses.

---

### Phase B1 ‚Äì Implement `ipc` package in Go (PoC mode)

**Goal:** When tools in the Go MCP server are called, they emit IPC requests using a stable abstraction.

#### Todo List - Phase B1

- [ ] **Task B1.1: Design IPC message structures**
  - [ ] Define Request message structure (type, id, tool, payload)
  - [ ] Define Response message structure (type, id, payload, error)
  - [ ] Document the IPC protocol in `docs/ipc-protocol.md`
  - [ ] Decide on transport mechanism for PoC (stdout/stdin recommended)

- [ ] **Task B1.2: Create IPC package structure**
  - [ ] Create `internal/ipc/` directory
  - [ ] Create `internal/ipc/ipc.go` file
  - [ ] Add necessary imports (encoding/json, io, uuid, etc.)

- [ ] **Task B1.3: Implement IPC client**
  - [ ] Define Request struct with JSON tags
  - [ ] Define Response struct with JSON tags
  - [ ] Create Client struct with writer (initially os.Stdout)
  - [ ] Implement `NewStdoutClient()` constructor
  - [ ] Implement `SendFireAndForget()` method with UUID generation
  - [ ] Add JSON marshaling and line-delimited output
  - [ ] Add basic error handling

   Implementation reference:

   ```go
   package ipc

   import (
       "encoding/json"
       "fmt"
       "github.com/google/uuid"
       "io"
       "os"
   )

   type Request struct {
       Type    string      `json:"type"` // "request"
       ID      string      `json:"id"`
       Tool    string      `json:"tool"`
       Payload interface{} `json:"payload"`
   }

   type Response struct {
       Type    string      `json:"type"` // "response"
       ID      string      `json:"id"`
       Payload interface{} `json:"payload"`
       Error   *string     `json:"error"`
   }

   type Client struct {
       w io.Writer // for PoC: os.Stdout
   }

   func NewStdoutClient() *Client {
       return &Client{w: os.Stdout}
   }

   func (c *Client) SendFireAndForget(tool string, payload interface{}) (string, error) {
       id := uuid.New().String()
       req := Request{
           Type:    "request",
           ID:      id,
           Tool:    tool,
           Payload: payload,
       }
       b, err := json.Marshal(req)
       if err != nil {
           return "", err
       }
       if _, err := fmt.Fprintln(c.w, string(b)); err != nil {
           return "", err
       }
       return id, nil
   }
   ```

- [ ] **Task B1.4: Integrate IPC client with MCP tools**
  - [ ] Create global IPC client instance in `cmd/mcp-server/main.go`
  - [ ] Update tool handlers to call IPC client
  - [ ] Pass tool name and input payload to IPC client
  - [ ] Handle IPC errors appropriately
  - [ ] Update tool responses to indicate IPC was sent

   Integration example:

   ```go
   var ipcClient = ipc.NewStdoutClient()

   func StartModuleTool(
       ctx context.Context,
       req *mcp.CallToolRequest,
       input StartModuleInput,
   ) (*mcp.CallToolResult, StartModuleOutput, error) {
       _, err := ipcClient.SendFireAndForget("start_module", input)
       if err != nil {
           return nil, StartModuleOutput{}, err
       }

       return nil, StartModuleOutput{
           Status: "IPC sent for " + input.ModuleName,
       }, nil
   }
   ```

- [ ] **Task B1.5: Test IPC output**
  - [ ] Build the updated MCP server binary
  - [ ] Run the MCP server from a terminal (not via Claude)
  - [ ] Use a test client or manual JSON-RPC calls to trigger tools
  - [ ] Verify JSON IPC messages are printed to stdout
  - [ ] Verify message format matches the defined protocol
  - [ ] Verify UUIDs are unique for each request
  - [ ] Save sample output for documentation

- [ ] **Task B1.6: Document IPC implementation**
  - [ ] Update `docs/ipc-protocol.md` with actual implementation details
  - [ ] Add examples of request and response messages
  - [ ] Document how to test IPC output manually
  - [ ] Add troubleshooting notes

**Exit criteria Phase B1:**
‚úÖ Go MCP tools, when called, emit JSON IPC messages (to stdout or a file). This is the first working PoC.

---

### Phase B2 ‚Äì Simple local bridge process (no LabVIEW yet)

**Goal:** Close the loop: something reads those IPC messages and sends back dummy responses.

You can do this in Go or Python; Go keeps the stack consistent.

#### Todo List - Phase B2

- [ ] **Task B2.1: Create bridge project structure**
  - [ ] Create `cmd/labview-bridge/` directory
  - [ ] Create `cmd/labview-bridge/main.go` file
  - [ ] Import necessary packages (bufio, encoding/json, internal/ipc)

- [ ] **Task B2.2: Implement bridge main loop**
  - [ ] Set up stdin scanner for reading line-delimited input
  - [ ] Create loop to continuously read input lines
  - [ ] Parse each line as IPC Request JSON
  - [ ] Add error handling for malformed JSON
  - [ ] Log received requests with tool name and payload

   Implementation skeleton:

   * Reads lines from stdin.
   * Parses each `ipc.Request`.
   * Logs it.
   * Writes back a matching `ipc.Response`.

   Sketch:

   ```go
   package main

   import (
       "bufio"
       "encoding/json"
       "fmt"
       "log"
       "os"

       "github.com/MeKo-Christian/labview_mcp_go/internal/ipc"
   )

   func main() {
       scanner := bufio.NewScanner(os.Stdin)
       for scanner.Scan() {
           line := scanner.Text()
           var req ipc.Request
           if err := json.Unmarshal([]byte(line), &req); err != nil {
               log.Println("bad request:", err)
               continue
           }

           log.Printf("Received tool=%s payload=%+v", req.Tool, req.Payload)

           // PoC: just echo back a canned response
           resp := ipc.Response{
               Type:  "response",
               ID:    req.ID,
               Error: nil,
               Payload: map[string]any{
                   "status": "handled " + req.Tool,
               },
           }

           b, _ := json.Marshal(resp)
           fmt.Println(string(b))
       }

       if err := scanner.Err(); err != nil {
           log.Println("scanner error:", err)
       }
   }
   ```

- [ ] **Task B2.3: Implement response generation**
  - [ ] Create canned response data for each tool type
  - [ ] Marshal response as IPC Response JSON
  - [ ] Write response to stdout (line-delimited)
  - [ ] Match request ID in response
  - [ ] Handle errors gracefully with error field in response

- [ ] **Task B2.4: Build and test bridge standalone**
  - [ ] Build bridge binary: `go build -o bin/labview-bridge ./cmd/labview-bridge`
  - [ ] Run bridge in a terminal
  - [ ] Manually pipe test IPC request JSON to stdin
  - [ ] Verify bridge receives and logs the request
  - [ ] Verify bridge outputs response JSON to stdout
  - [ ] Test with multiple requests

- [ ] **Task B2.5: Integrate bridge with MCP server**
  - [ ] Option A: Run bridge in separate terminal and pipe MCP output to it
  - [ ] Option B: Modify MCP server to launch bridge as child process
  - [ ] If Option B: Connect bridge stdin/stdout pipes to MCP server
  - [ ] If Option B: Implement request/response correlation with channels
  - [ ] Test end-to-end: Claude ‚Üí MCP Server ‚Üí Bridge ‚Üí Response

- [ ] **Task B2.6: Future-proof for bidirectional communication**
  - [ ] Design response correlation mechanism (map[id]chan Response)
  - [ ] Consider timeout handling for requests
  - [ ] Document upgrade path from fire-and-forget to request-response
  - [ ] Add notes to `docs/ipc-protocol.md`

   Note: For the PoC, you can still keep it one-way, but to be future-proof:

   * Have the MCP server start `labview-bridge` as a child process.
   * Connect to its stdin/stdout pipes.
   * Maintain a `map[id]chan Response` to correlate responses.

   (This can be Phase B3 if you don't want to go that deep yet.)

**Exit criteria Phase B2:**
‚úÖ You can:

- Run `labview-bridge` in one console
- Pipe IPC logs from the Go MCP server into it (or launch it as a child)
- See IPC requests and responses flowing, even though LabVIEW isn't involved yet
- Verify the complete round-trip communication works

At that point, you've **fully validated the IPC design**.

---

### Phase B3 ‚Äì Replace stub bridge logic with real LabVIEW calls (later)

This is where COM / LabVIEW integration goes, but it's **behind** the `labview-bridge` process and `ipc` package.

**Goal:** Connect the bridge to actual LabVIEW functionality while keeping the Go MCP server unchanged.

#### Todo List - Phase B3

**Note:** The Go MCP server doesn't care whether the bridge uses Python + pywin32, Go + COM, TCP into a LabVIEW-built "server VI", or any other mechanism. Choose the approach that works best for your environment.

**Approach A: Call Python from Go bridge (Recommended for quick start)**

- [ ] **Task B3.A1: Integrate existing Python code**
  - [ ] Create `internal/bridge/python_adapter.go`
  - [ ] Implement function to call Python scripts via `exec.Command`
  - [ ] Pass tool name and payload as arguments to Python script
  - [ ] Capture Python script output as response
  - [ ] Handle errors from Python execution

- [ ] **Task B3.A2: Adapt existing Python LabVIEW code**
  - [ ] Extract LabVIEW COM interaction code from `main.py`
  - [ ] Create standalone Python scripts for each tool (e.g., `scripts/start_module.py`)
  - [ ] Each script should read JSON input from stdin or arguments
  - [ ] Each script should output JSON response to stdout
  - [ ] Test scripts independently before bridge integration

- [ ] **Task B3.A3: Wire Python adapter into bridge**
  - [ ] Update bridge main loop to call Python adapter instead of canned responses
  - [ ] Map tool names to Python script paths
  - [ ] Pass request payload to Python script
  - [ ] Parse Python output as response
  - [ ] Add error handling for Python script failures

**Approach B: Direct Go + COM integration (Advanced)**

- [ ] **Task B3.B1: Research Go COM libraries**
  - [ ] Evaluate `github.com/go-ole/go-ole` for Windows COM
  - [ ] Test basic COM connectivity to LabVIEW
  - [ ] Document COM API requirements for LabVIEW integration

- [ ] **Task B3.B2: Implement COM wrapper**
  - [ ] Create `internal/bridge/labview_com.go`
  - [ ] Implement COM initialization and connection
  - [ ] Implement functions to call LabVIEW VIs via COM
  - [ ] Add proper error handling and resource cleanup
  - [ ] Test COM calls independently

- [ ] **Task B3.B3: Integrate COM into bridge**
  - [ ] Update bridge to use COM wrapper instead of canned responses
  - [ ] Handle COM connection lifecycle (connect on start, disconnect on exit)
  - [ ] Add retry logic for COM failures
  - [ ] Test end-to-end with real LabVIEW

**Approach C: TCP/HTTP to LabVIEW server VI**

- [ ] **Task B3.C1: Create LabVIEW TCP/HTTP server**
  - [ ] Design LabVIEW VI that listens on TCP or HTTP
  - [ ] Implement request parsing (JSON)
  - [ ] Implement tool routing to appropriate VIs
  - [ ] Implement response generation (JSON)
  - [ ] Test server VI independently

- [ ] **Task B3.C2: Implement Go TCP/HTTP client**
  - [ ] Create `internal/bridge/labview_client.go`
  - [ ] Implement HTTP/TCP client to communicate with LabVIEW server
  - [ ] Add connection pooling and timeout handling
  - [ ] Add retry logic for network failures

- [ ] **Task B3.C3: Integrate client into bridge**
  - [ ] Update bridge to use LabVIEW client instead of canned responses
  - [ ] Handle connection lifecycle
  - [ ] Test end-to-end with LabVIEW server VI

**Common tasks for all approaches:**

- [ ] **Task B3.X1: Test with real LabVIEW integration**
  - [ ] Start LabVIEW environment
  - [ ] Run bridge with real LabVIEW backend
  - [ ] Test each tool through the full stack: Claude ‚Üí MCP ‚Üí Bridge ‚Üí LabVIEW
  - [ ] Verify responses match expected formats
  - [ ] Test error cases (LabVIEW not running, invalid inputs, etc.)

- [ ] **Task B3.X2: Performance and reliability improvements**
  - [ ] Add logging for LabVIEW interaction
  - [ ] Implement timeouts for LabVIEW calls
  - [ ] Add health checks for LabVIEW connectivity
  - [ ] Document performance characteristics
  - [ ] Add monitoring/observability

- [ ] **Task B3.X3: Documentation and deployment**
  - [ ] Document chosen integration approach
  - [ ] Create setup guide for LabVIEW environment
  - [ ] Document dependencies (Python, COM libraries, etc.)
  - [ ] Create deployment checklist
  - [ ] Update README with installation instructions

**Exit criteria Phase B3:**
‚úÖ The bridge successfully calls real LabVIEW functionality, and the entire stack works end-to-end from Claude to LabVIEW and back.

---

## Suggested order of implementation

### Implementation Roadmap

This is the recommended sequence for transforming this repository. Each phase builds on the previous one.

#### Track A: Go MCP Server (Core)

- [ ] **Phase A1:** Set up the Go project & skeleton MCP server
  - [ ] Initialize Go module in this repository
  - [ ] Create minimal MCP server with `ping` tool
  - [ ] Configure Claude Desktop to use Go server
  - [ ] Verify basic connectivity

- [ ] **Phase A2:** Extract tool schema from current system
  - [ ] Analyze existing Python implementation
  - [ ] Document all tool schemas
  - [ ] Prioritize 2-3 tools for v1

- [ ] **Phase A3:** Implement Go tools as stubs
  - [ ] Implement v1 tools with stub responses
  - [ ] Deploy to Claude Desktop
  - [ ] Verify tool discovery and invocation

**Milestone:** At this point, Claude can call the Go MCP server with the same tools as Python, but without real LabVIEW integration yet.

---

#### Track B: IPC & LabVIEW Integration

- [ ] **Phase B1:** Implement IPC package (PoC mode)
  - [ ] Create `internal/ipc` package
  - [ ] Emit JSON IPC messages from tools
  - [ ] Verify IPC output format
  - *Result:* You now have an IPC-spitting PoC

- [ ] **Phase B2:** Simple local bridge process
  - [ ] Create `cmd/labview-bridge` binary
  - [ ] Implement request reading and canned responses
  - [ ] Test bridge standalone
  - [ ] Integrate bridge with MCP server
  - *Result:* Complete IPC round-trip with dummy data

- [ ] **Phase B3:** Real LabVIEW integration
  - [ ] Choose integration approach (Python adapter, COM, or TCP)
  - [ ] Implement chosen approach
  - [ ] Test with real LabVIEW environment
  - [ ] Performance tuning and error handling
  - *Result:* Full end-to-end functionality

**Milestone:** At this point, the entire transformation is complete. The Go-based MCP server successfully communicates with LabVIEW.

---

### Quick Start Guide

For developers starting this transformation:

1. **Start here:** Begin with Phase A1 (see detailed todo-list above)
2. **Checkpoint:** After A3, you have a working Go MCP server (without LabVIEW)
3. **Continue:** Proceed to B1 for IPC integration
4. **Final step:** Complete B3 for real LabVIEW integration

### Migration Notes

- Python implementation stays in the repository during development
- Both servers can coexist during testing
- Switch Claude Desktop config to toggle between Python and Go
- Only deprecate Python after B3 is complete and stable
